{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":193567451,"sourceType":"kernelVersion"},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900,"modelId":1902},{"sourceId":85994,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72253,"modelId":76277}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset, Dataset, concatenate_datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_essays = load_dataset('csv', data_files = '/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\ntest_essays = load_dataset('csv', data_files = '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\ntrain_prompts = load_dataset('csv', data_files = '/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_essays, test_essays, train_prompts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_split = train_essays['train'].train_test_split(test_size = 0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_test_split['train'].rename_column('generated' , 'label')\nval = train_test_split['test'].rename_column('generated' , 'label')\ntest = test_essays['train']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train , val, test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nmodel_ckpt = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_use_double_quant = True,\n    bnb_4bit_quant_type = 'nf4',\n    bnb_4bit_compute_dtype = torch.bfloat16\n)\n\ndef get_new_model():\n    return AutoModelForCausalLM.from_pretrained(model_ckpt, quantization_config = bnb_config, low_cpu_mem_usage = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts = ['<prompt> ' + ' '.join(text.split()[:-7]) for text in train_prompts['train']['instructions']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = [' '.join(text.split()[:10]) for text in train['text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_prompts(prompts, train_data):\n    return  [prompts[prompt_id] + ' <response> ' + text  for prompt_id, text in zip(train_data['prompt_id'], texts)]\n      \n    \ninput_texts = Dataset.from_dict({'text' : make_prompts(prompts, train)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'left'\n\ndef preprocess(batch):\n    return tokenizer(batch['text'], padding = 'max_length', truncation = True, return_tensors = 'pt', max_length = 128) \n\ninputs = input_texts.map(preprocess, batched = True, batch_size = 16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_new_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ngenerated_texts = []\nbatch_size = 32\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfor i in range(0, len(inputs), batch_size):\n    start = i; end = min(i + batch_size, len(inputs))\n    batch = inputs[start:end]\n    \n    input_ids =  torch.tensor(batch['input_ids']).to(device)\n    attention_mask =  torch.tensor(batch['attention_mask']).to(device)\n     \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            max_new_tokens = 1000,\n            top_p = 0.9,\n            top_k = 50,\n            temperature = 0.7,\n            repetition_penalty = 1.2,\n            do_sample = True\n        )\n    generated_texts.extend([tokenizer.decode(output, skip_sepcial_tokens = True) for output in outputs])\n    \n    print(f'batch {i // batch_size + 1} generation completed')\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_texts = [text.replace(tokenizer.pad_token, '').split('<response>')[1].strip() for text in generated_texts]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_data = Dataset.from_dict({'text' : cleaned_texts, 'label' : [1 for _ in range(len(cleaned_texts))]})\ntrain_ = train.remove_columns(['id','prompt_id'])\ntrain_data = concatenate_datasets([train_, gen_data])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast\nimport torch\n\ngemma_model_ckpt = '/kaggle/input/gemma-2/transformers/gemma-2-2b/1'\ngemma_tokenizer = GemmaTokenizerFast.from_pretrained(gemma_model_ckpt)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_use_double_quant = True,\n    bnb_4bit_quant_type = 'nf4',\n    bnb_4bit_compute_dtype = torch.bfloat16\n)\n\ndef get_classifier():\n    return Gemma2ForSequenceClassification.from_pretrained(gemma_model_ckpt, num_labels = 2, quantization_config = bnb_config, low_cpu_mem_usage = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_data(batch):\n    return gemma_tokenizer(batch['text'], padding = True, truncation = True, max_length = 512)\n\ntrain_ds = train_data.map(tokenize_data, batched = True)\nval_ds = val.map(tokenize_data, batched = True)\ntest_ds = test.map(tokenize_data, batched = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds, test_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['WANDB_DISABLED'] = 'true'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds)\n    return {'f1' : f1}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"per_device_bs =  2\noutput_dir = '/kaggle/working/gemma2'\n\nargs = TrainingArguments(\n    output_dir = output_dir,\n    num_train_epochs = 2,\n    learning_rate = 1e-5,\n    per_device_train_batch_size = per_device_bs,\n    per_device_eval_batch_size = per_device_bs,\n    eval_strategy = 'epoch',\n    save_strategy = 'epoch',\n    logging_steps = 100,\n    load_best_model_at_end = True,\n    overwrite_output_dir = True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = get_classifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n\nlora_config = LoraConfig(\n    r = 8,\n    lora_alpha = 16,\n    target_modules = ['q_proj','v_proj','k_proj', 'o_proj'],\n    bias = 'none',\n    task_type = 'SEQ_CLS',\n    init_lora_weights = True,\n)\n\npeft_model =  get_peft_model(classifier, lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model = peft_model,\n    args = args,\n    train_dataset = train_ds,\n    eval_dataset = val_ds,\n    compute_metrics = compute_metrics,\n    tokenizer = gemma_tokenizer\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = trainer.predict(test_ds)\n\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.sigmoid(torch.tensor(preds.predictions))\n\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nsub = pd.DataFrame({\n    'id' : test_['train']['id'],\n    'generated' : x[:, 0]\n})\n\nsub.to_csv('submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil \n\n\nshutil.rmtree('/kaggle/working/bert')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}